
#################convert model########################
python convert_tf_checkpoint_to_pytorch.py \
--tf_checkpoint_path bert-model/uncased_L-12_H-768_A-12/bert_model.ckpt \
--bert_config_file bert-model/uncased_L-12_H-768_A-12/bert_config.json \
--pytorch_dump_path bert-model/uncased_L-12_H-768_A-12/pytorch_model.bin



#################GlossBERT_sent_cls_ws##########################

## train
CUDA_VISIBLE_DEVICES=0,1,2,3 python run_classifier_WSD_sent.py \
--task_name WSD \
--train_data_dir ./Training_Corpora/SemCor/semcor_train_sent_cls_ws.csv \
--eval_data_dir ./Evaluation_Datasets/semeval2007/semeval2007_test_sent_cls_ws.csv \
--output_dir results/sent_cls_ws/1314 \
--bert_model ./bert-model/uncased_L-12_H-768_A-12/ \
--do_train \
--do_eval \
--do_lower_case \
--max_seq_length 512 \
--train_batch_size 64 \
--eval_batch_size 128 \
--learning_rate 2e-5 \
--num_train_epochs 6.0 \
--seed 1314

## test
CUDA_VISIBLE_DEVICES=0,1,2,3 python run_classifier_WSD_sent.py \
--task_name WSD \
--eval_data_dir ./Evaluation_Datasets/senseval3/senseval3_test_sent_cls_ws.csv \
--output_dir results/sent_cls_ws/1314/4 \
--bert_model results/sent_cls_ws/1314/4 \
--do_test \
--do_lower_case \
--max_seq_length 512 \
--train_batch_size 64 \
--eval_batch_size 128 \
--learning_rate 2e-5 \
--num_train_epochs 6.0 \
--seed 1314

#################GlossBERT_sent_cls##########################

## train
CUDA_VISIBLE_DEVICES=0,1,2,3 python run_classifier_WSD_sent.py \
--task_name WSD \
--train_data_dir ./Training_Corpora/SemCor/semcor_train_sent_cls.csv \
--eval_data_dir ./Evaluation_Datasets/semeval2007/semeval2007_test_sent_cls.csv \
--output_dir results/sent_cls/1314 \
--bert_model ./bert-model/uncased_L-12_H-768_A-12/ \
--do_train \
--do_eval \
--do_lower_case \
--max_seq_length 512 \
--train_batch_size 64 \
--eval_batch_size 128 \
--learning_rate 2e-5 \
--num_train_epochs 6.0 \
--seed 1314

## test
CUDA_VISIBLE_DEVICES=0,1,2,3 python run_classifier_WSD_sent.py \
--task_name WSD \
--eval_data_dir ./Evaluation_Datasets/senseval3/senseval3_test_sent_cls.csv \
--output_dir results/sent_cls/1314/4 \
--bert_model results/sent_cls/1314/4 \
--do_test \
--do_lower_case \
--max_seq_length 512 \
--train_batch_size 64 \
--eval_batch_size 128 \
--learning_rate 2e-5 \
--num_train_epochs 6.0 \
--seed 1314

#################GlossBERT_token_cls##########################

## train
CUDA_VISIBLE_DEVICES=0,1,2,3 python run_classifier_WSD_token.py \
--task_name WSD \
--train_data_dir ./Training_Corpora/SemCor/semcor_train_token_cls.csv \
--eval_data_dir ./Evaluation_Datasets/semeval2007/semeval2007_test_token_cls.csv \
--label_data_dir ./wordnet/ \
--output_dir results/token/1314 \
--bert_model ./bert-model/uncased_L-12_H-768_A-12/ \
--do_train \
--do_eval \
--do_lower_case \
--max_seq_length 512 \
--train_batch_size 64 \
--eval_batch_size 128 \
--learning_rate 2e-5 \
--num_train_epochs 6.0 \
--seed 1314

## test
CUDA_VISIBLE_DEVICES=0,1,2,3 python run_classifier_WSD_token.py \
--task_name WSD \
--eval_data_dir ./Evaluation_Datasets/senseval3/senseval3_test_token_cls.csv \
--label_data_dir ./wordnet/ \
--output_dir results/token/1314/4 \
--bert_model results/token/1314/4 \
--do_test \
--do_lower_case \
--max_seq_length 512 \
--train_batch_size 64 \
--eval_batch_size 128 \
--learning_rate 2e-5 \
--num_train_epochs 6.0 \
--seed 1314


#################BERT_baseline##########################

## train
CUDA_VISIBLE_DEVICES=0,1,2,3 python run_classifier_WSD_baseline.py \
--task_name WSD \
--train_data_dir ./Training_Corpora/SemCor/semcor.csv \
--eval_data_dir ./Evaluation_Datasets/semeval2007/semeval2007.csv \
--label_data_dir ./wordnet/ \
--output_dir results/baseline/1314 \
--bert_model ./bert-model/uncased_L-12_H-768_A-12/ \
--do_train \
--do_eval \
--do_lower_case \
--max_seq_length 512 \
--train_batch_size 64 \
--eval_batch_size 128 \
--learning_rate 2e-5 \
--num_train_epochs 6.0 \
--seed 1314

## test
CUDA_VISIBLE_DEVICES=0,1,2,3 python run_classifier_WSD_baseline.py \
--task_name WSD \
--eval_data_dir ./Evaluation_Datasets/senseval3/senseval3.csv \
--label_data_dir ./wordnet/ \
--output_dir results/baseline/1314/4 \
--bert_model results/baseline/1314/4 \
--do_test \
--do_lower_case \
--max_seq_length 512 \
--train_batch_size 64 \
--eval_batch_size 128 \
--learning_rate 2e-5 \
--num_train_epochs 6.0 \
--seed 1314


#######################convert result baseline/token/sent#################3
## GlossBERT_sent_cls or GlossBERT_sent_cls_ws or GlossBERT_token_cls
python convert_result_token_sent.py \
--dataset semeval2007 \
--input_file ./results/results.txt \
--output_dir ./results/  

## BERT_baseline
python convert_result_baseline.py \
--dataset semeval2007 \
--input_file ./results/results.txt \
--output_dir ./results/  


#####################scorer########################
java Scorer ./Evaluation_Datasets/semeval2007/semeval2007.gold.key.txt ./results/final_result_semeval2007.txt
